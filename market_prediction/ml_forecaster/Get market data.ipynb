{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f0c3e7-98ea-492b-99bd-b5b5356d0916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade -Uqqq mlflow>=3.0 xgboost optuna uv\n",
    "\n",
    "from pipeline import run_pipeline\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#model, metrics = run_pipeline(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f696441d-264f-4625-806d-67b84a4fe9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_pipeline(\n",
    "    spark,\n",
    "    table_name=\"AAPL_market_price\",\n",
    "    n_lags=10,\n",
    "    n_steps=3,\n",
    "    as_direction=True,\n",
    "    test_size=0.2,\n",
    "    shuffle=False,\n",
    "    return_type=\"dataframe\",\n",
    "    verbose=True,\n",
    "    experiment_name=\"/Users/feldmanngreg@gmail.com/AAPL_Forecaster\",\n",
    "    register_model_name=\"main.default.updown_forecaster_model\"\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for loading, training, evaluating, and logging a forecasting model.\n",
    "    Returns:\n",
    "        model: trained classifier\n",
    "        metrics: dict of accuracy metrics\n",
    "    \"\"\"\n",
    "    from data_loader import load_adj_close_from_spark_table\n",
    "    from features import create_multistep_lagged_numpy\n",
    "    from modelling import split_data, train_model, predict, evaluate\n",
    "\n",
    "    # Set the experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Step 1: Load data\n",
    "    historical_data = load_adj_close_from_spark_table(spark, table_name=table_name, return_type=return_type)\n",
    "\n",
    "    # Step 2: Create lagged features and labels\n",
    "    X, y = create_multistep_lagged_numpy(\n",
    "        historical_data[\"adj_close\"].values,\n",
    "        n_lags=n_lags,\n",
    "        n_steps=n_steps,\n",
    "        as_direction=as_direction\n",
    "    )\n",
    "\n",
    "    # Step 3: Train-test split\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # Step 4: Train the model\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Step 5: Predict\n",
    "    y_pred = predict(model, X_test)\n",
    "\n",
    "    # Step 6: Evaluate\n",
    "    metrics = evaluate(y_test, y_pred, verbose=verbose)\n",
    "\n",
    "    # Step 7: Prepare MLflow inputs\n",
    "    X_test_df = pd.DataFrame(X_test, columns=[f\"lag_{i}\" for i in range(X_test.shape[1])])\n",
    "    evaluation_data = X_test_df.copy()\n",
    "    evaluation_data[\"label\"] = y_test[:, 0]  # only evaluating t+1\n",
    "\n",
    "    signature = infer_signature(X_test_df, y_pred)\n",
    "\n",
    "    # Step 8: Log to MLflow\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_param(\"n_lags\", n_lags)\n",
    "        mlflow.log_param(\"n_steps\", n_steps)\n",
    "        mlflow.log_param(\"model_type\", \"RandomForestClassifier\")\n",
    "\n",
    "        mlflow.log_metric(\"overall_accuracy\", metrics[\"overall_accuracy\"])\n",
    "        for i, acc in enumerate(metrics[\"per_step_accuracy\"], 1):\n",
    "            mlflow.log_metric(f\"accuracy_t+{i}\", acc)\n",
    "\n",
    "        model_info = mlflow.sklearn.log_model(model,\n",
    "            name=\"sk_models\",\n",
    "            input_example=X_test_df.iloc[[0]],\n",
    "            signature=signature,\n",
    "            registered_model_name=register_model_name\n",
    "        )\n",
    "\n",
    "        # Optional: log feature importance\n",
    "        try:\n",
    "            importances = model.estimators_[0].feature_importances_\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.bar(range(len(importances)), importances)\n",
    "            ax.set_title(\"Feature Importance (t+1 estimator)\")\n",
    "            mlflow.log_figure(fig, \"feature_importance_t+1.png\")\n",
    "            plt.close(fig)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot feature importance: {e}\")\n",
    "\n",
    "        # Evaluate using MLflow's evaluator (only on t+1 step)\n",
    "        mlflow.models.evaluate(\n",
    "            model=model_info.model_uri,\n",
    "            data=evaluation_data,\n",
    "            targets=\"label\",\n",
    "            model_type=\"classifier\",\n",
    "            evaluator_config={\"metric_prefix\": \"mlflow_eval_\"}\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Model logged and registered: {model_info.model_uri}\")\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4bcbfe-e737-4a4f-ad56-31d298647212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "table_name=\"AAPL_market_price\"\n",
    "n_lags=10\n",
    "n_steps=3\n",
    "as_direction=True\n",
    "test_size=0.2\n",
    "shuffle=False\n",
    "return_type=\"dataframe\"\n",
    "verbose=True\n",
    "experiment_name=\"/Users/feldmanngreg@gmail.com/AAPL_Forecaster\"\n",
    "register_model_name=\"main.default.updown_forecaster_model\"\n",
    "\n",
    "from data_loader import load_adj_close_from_spark_table\n",
    "from features import create_multistep_lagged_numpy\n",
    "from modelling import split_data, train_model, predict, evaluate\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Step 1: Load data\n",
    "historical_data = load_adj_close_from_spark_table(spark, table_name=table_name, return_type=return_type)\n",
    "\n",
    "# Step 2: Create lagged features and labels\n",
    "X, y = create_multistep_lagged_numpy(\n",
    "    historical_data[\"adj_close\"].values,\n",
    "    n_lags=n_lags,\n",
    "    n_steps=n_steps,\n",
    "    as_direction=as_direction\n",
    ")\n",
    "\n",
    "# Step 3: Train-test split\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "# Step 4: Train the model\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict\n",
    "y_pred = predict(model, X_test)\n",
    "\n",
    "# Step 6: Evaluate\n",
    "metrics = evaluate(y_test, y_pred, verbose=verbose)\n",
    "\n",
    "# Step 7: Prepare MLflow inputs\n",
    "X_test_df = pd.DataFrame(X_test, columns=[f\"lag_{i}\" for i in range(X_test.shape[1])])\n",
    "evaluation_data = X_test_df.copy()\n",
    "evaluation_data[\"label\"] = y_test[:, 0]  # only evaluating t+1\n",
    "\n",
    "signature = infer_signature(X_test_df, y_pred)\n",
    "\n",
    "# Step 8: Log to MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_param(\"n_lags\", n_lags)\n",
    "    mlflow.log_param(\"n_steps\", n_steps)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestClassifier\")\n",
    "\n",
    "    mlflow.log_metric(\"overall_accuracy\", metrics[\"overall_accuracy\"])\n",
    "    for i, acc in enumerate(metrics[\"per_step_accuracy\"], 1):\n",
    "        mlflow.log_metric(f\"accuracy_t+{i}\", acc)\n",
    "\n",
    "    model_info = mlflow.sklearn.log_model(model,\n",
    "        name=\"sk_models\",\n",
    "        input_example=X_test_df.iloc[[0]],\n",
    "        signature=signature,\n",
    "        registered_model_name=register_model_name\n",
    "    )\n",
    "\n",
    "    # Optional: log feature importance\n",
    "    try:\n",
    "        importances = model.estimators_[0].feature_importances_\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.bar(range(len(importances)), importances)\n",
    "        ax.set_title(\"Feature Importance (t+1 estimator)\")\n",
    "        mlflow.log_figure(fig, \"feature_importance_t+1.png\")\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot feature importance: {e}\")\n",
    "\n",
    "    # Evaluate using MLflow's evaluator (only on t+1 step)\n",
    "    for i in range(y_test.shape[1]):\n",
    "        mlflow.log_metric(f\"accuracy_step_{i+1}\", np.mean(y_test[:, i] == y_pred[:, i]))\n",
    "\n",
    "    print(f\"✅ Model logged and registered: {model_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9439144d-b7ec-4f30-b010-f1c55502bccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c067805e-b24f-4cf2-8070-9446205e83b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3123069d-6c17-4c16-abdb-789fb5253ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Get market data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
