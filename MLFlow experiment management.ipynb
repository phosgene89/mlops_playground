{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcf2e47-de02-441a-85b7-c4ccbed4dbd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  XGBoost MLflow tutorial\n",
    "\n",
    "This tutorial covers the full lifecycle of experimentation, training, tuning, registration, evaluation, and deployment for a classic ML modeling project. It shows you how to use MLflow to keep track of every aspect of the model development and deployment processes.\n",
    "\n",
    "In this step-by-step tutorial, you'll discover how to:\n",
    "- **Generate and visualize data:** Create synthetic data to simulate real-world scenarios, and visualize feature relationships with Seaborn.\n",
    "- **Train and log models:** Train an XGBoost model, and log important metrics, parameters, and artifacts using MLflow, including visualizations.\n",
    "- **Register models:** Register your model with Unity Catalog, preparing it for review and future deployment to managed serving endpoints.\n",
    "- **Load and evaluate models:** Load your registered model, make predictions, and perform error analysis to validate model performance.\n",
    "\n",
    "This tutorial leverages features from Mlflow 3.0. For more details, see \"Get started with MLflow 3.0\" ([AWS](https://docs.databricks.com/aws/en/mlflow/mlflow-3-install)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/mlflow/mlflow-3-install)|[GCP](https://docs.databricks.com/gcp/en/mlflow/mlflow-3-install))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2597cd0-39e8-42ae-8af2-caa32b12985e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install the latest version of MLflow"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade -Uqqq mlflow>=3.0 xgboost optuna uv\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "079526af-325c-453d-a450-9fbe208702d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e453b9-5ade-4564-a0d8-dbed80352888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0. Configure the Model Registry with Unity Catalog\n",
    "\n",
    "One of the key advantages of using MLflow on Databricks is the seamless integration with **Unity Catalog**. This integration simplifies model management and governance, ensuring that every model you develop is tracked, versioned, and secure. For more information about Unity Catalog, see ([AWS](https://docs.databricks.com/aws/en/data-governance/unity-catalog) | [Azure](https://learn.microsoft.com/azure/databricks/data-governance/unity-catalog) | [GCP](https://docs.databricks.com/gcp/en/data-governance/unity-catalog)).\n",
    "\n",
    "### Set the registry URI\n",
    "\n",
    "The following cell configures MLflow to use Unity Catalog for model registration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "714cdcf1-19ce-4e6d-8c90-98c19784a123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c8e12b-744a-4c57-83b7-97b3be876830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Create a synthetic regression dataset\n",
    "\n",
    "The next cell defines the `create_regression_data` function. This function generates synthetic data for regression. The resulting DataFrame includes correlated data, cyclical patterns, and outliers. These features are designed to mimic real-world data scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa44c59-08e9-4a71-84df-3af30f7c7d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_regression_data(\n",
    "    n_samples: int, \n",
    "    n_features: int,\n",
    "    seed: int = 1994,\n",
    "    noise_level: float = 0.3,\n",
    "    nonlinear: bool = True\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Generates synthetic regression data with interesting correlations for MLflow and XGBoost demonstrations.\n",
    "\n",
    "    This function creates a DataFrame of continuous features and computes a target variable with nonlinear\n",
    "    relationships and interactions between features. The data is designed to be complex enough to demonstrate\n",
    "    the capabilities of XGBoost, but not so complex that a reasonable model can't be learned.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Number of samples (rows) to generate.\n",
    "        n_features (int): Number of feature columns.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 1994.\n",
    "        noise_level (float, optional): Level of Gaussian noise to add to the target. Defaults to 0.3.\n",
    "        nonlinear (bool, optional): Whether to add nonlinear feature transformations. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.Series]:\n",
    "            - pd.DataFrame: DataFrame containing the synthetic features.\n",
    "            - pd.Series: Series containing the target labels.\n",
    "\n",
    "    Example:\n",
    "        >>> df, target = create_regression_data(n_samples=1000, n_features=10)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Generate random continuous features\n",
    "    X = rng.uniform(-5, 5, size=(n_samples, n_features))\n",
    "    \n",
    "    # Create feature DataFrame with meaningful names\n",
    "    columns = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    \n",
    "    # Generate base target variable with linear relationship to a subset of features\n",
    "    # Use only the first n_features//2 features to create some irrelevant features\n",
    "    weights = rng.uniform(-2, 2, size=n_features//2)\n",
    "    target = np.dot(X[:, :n_features//2], weights)\n",
    "    \n",
    "    # Add some nonlinear transformations if requested\n",
    "    if nonlinear:\n",
    "        # Add square term for first feature\n",
    "        target += 0.5 * X[:, 0]**2\n",
    "        \n",
    "        # Add interaction between the second and third features\n",
    "        if n_features >= 3:\n",
    "            target += 1.5 * X[:, 1] * X[:, 2]\n",
    "        \n",
    "        # Add sine transformation of fourth feature\n",
    "        if n_features >= 4:\n",
    "            target += 2 * np.sin(X[:, 3])\n",
    "        \n",
    "        # Add exponential of fifth feature, scaled down\n",
    "        if n_features >= 5:\n",
    "            target += 0.1 * np.exp(X[:, 4] / 2)\n",
    "            \n",
    "        # Add threshold effect for sixth feature\n",
    "        if n_features >= 6:\n",
    "            target += 3 * (X[:, 5] > 1.5).astype(float)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = rng.normal(0, noise_level * target.std(), size=n_samples)\n",
    "    target += noise\n",
    "    \n",
    "    # Add a few more interesting features to the DataFrame\n",
    "    \n",
    "    # Add a correlated feature (but not used in target calculation)\n",
    "    if n_features >= 7:\n",
    "        df['feature_correlated'] = df['feature_0'] * 0.8 + rng.normal(0, 0.2, size=n_samples)\n",
    "    \n",
    "    # Add a cyclical feature\n",
    "    df['feature_cyclical'] = np.sin(np.linspace(0, 4*np.pi, n_samples))\n",
    "    \n",
    "    # Add a feature with outliers\n",
    "    df['feature_with_outliers'] = rng.normal(0, 1, size=n_samples)\n",
    "    # Add outliers to ~1% of samples\n",
    "    outlier_idx = rng.choice(n_samples, size=n_samples//100, replace=False)\n",
    "    df.loc[outlier_idx, 'feature_with_outliers'] = rng.uniform(10, 15, size=len(outlier_idx))\n",
    "    \n",
    "    return df, pd.Series(target, name='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b9be73-11c2-4927-a5db-2b387505d065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Exploratory data analysis (EDA) visualizations\n",
    "\n",
    "Before training your model, it’s essential to examine your data. Visualizations help you validate that the data is as expected, spot unexpected anomalies, and drive feature selection. As you move forward with model development, these visualizations serve as a record of your work that can help with troubleshooting, reproducibility, and collaboration.  \n",
    "\n",
    "You can use MLflow to log visualizations, making your experimentation fully reproducible.  \n",
    "\n",
    "The code in the following cell creates 6 functions, each of which generates a different plot to help you visually inspect your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b6f64e-b31e-449e-a41f-b20c836ad7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_distributions(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of histograms for each feature in the dataset.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing synthetic features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the distribution plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            sns.histplot(X[feature], ax=ax, kde=True, color='skyblue')\n",
    "            ax.set_title(f'Distribution of {feature}')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature Distributions', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_heatmap(X: pd.DataFrame, y: pd.Series) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a correlation heatmap of all features and the target variable.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the heatmap.\n",
    "    \"\"\"\n",
    "    # Combine features and target into one DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Draw the heatmap with a color bar\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap=cmap,\n",
    "                center=0, square=True, linewidths=0.5, ax=ax)\n",
    "    \n",
    "    ax.set_title('Feature Correlation Heatmap', fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_feature_target_relationships(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of scatter plots showing the relationship between each feature and the target.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the relationship plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Scatter plot with regression line\n",
    "            sns.regplot(x=X[feature], y=y, ax=ax, \n",
    "                       scatter_kws={'alpha': 0.5, 'color': 'blue'}, \n",
    "                       line_kws={'color': 'red'})\n",
    "            ax.set_title(f'{feature} vs Target')\n",
    "    \n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature vs Target Relationships', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_pairwise_relationships(X: pd.DataFrame, y: pd.Series, features: list[str]) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a pairplot showing relationships between selected features and the target.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        features (List[str]): List of feature names to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the pairplot.\n",
    "    \"\"\"\n",
    "    # Ensure features exist in the DataFrame\n",
    "    valid_features = [f for f in features if f in X.columns]\n",
    "    \n",
    "    if not valid_features:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.text(0.5, 0.5, \"No valid features provided\", ha='center', va='center')\n",
    "        return fig\n",
    "    \n",
    "    # Combine selected features and target\n",
    "    data = X[valid_features].copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Create pairplot\n",
    "    pairgrid = sns.pairplot(data, diag_kind=\"kde\", \n",
    "                          plot_kws={\"alpha\": 0.6, \"s\": 50},\n",
    "                          corner=True)\n",
    "    \n",
    "    pairgrid.fig.suptitle(\"Pairwise Feature Relationships\", y=1.02, fontsize=16)\n",
    "    plt.close(pairgrid.fig)\n",
    "    return pairgrid.fig\n",
    "\n",
    "def plot_boxplots(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of box plots for each feature, with points colored by target value.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the box plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    # Create target bins for coloring\n",
    "    y_binned = pd.qcut(y, 3, labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Box plot for each feature\n",
    "            sns.boxplot(x=y_binned, y=X[feature], ax=ax)\n",
    "            ax.set_title(f'Distribution of {feature} by Target Range')\n",
    "            ax.set_xlabel('Target Range')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature Distributions by Target Range', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_outliers(X: pd.DataFrame, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of box plots to detect outliers in each feature.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the outlier plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Box plot to detect outliers\n",
    "            sns.boxplot(x=X[feature], ax=ax, color='skyblue')\n",
    "            ax.set_title(f'Outlier Detection for {feature}')\n",
    "            ax.set_xlabel(feature)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Outlier Detection for Features', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac87b21-1ca6-400a-8109-9e343211d4e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Standard modeling workflow\n",
    "\n",
    "The code in the next cell does the following:\n",
    "1. Uses the function you created, `create_regression_data`, to create a dataset.\n",
    "2. Uses the visualization functions you created to create EDA plots.\n",
    "3. Configures and trains an XGBoost model.\n",
    "4. Uses the trained model to make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f0d424-cab9-4f89-b8a5-f73ae56c2cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the regression dataset\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "X, y = create_regression_data(n_samples=n_samples, n_features=n_features, nonlinear=True)\n",
    "\n",
    "# Create EDA plots\n",
    "dist_plot = plot_feature_distributions(X, y)\n",
    "corr_plot = plot_correlation_heatmap(X, y)\n",
    "scatter_plot = plot_feature_target_relationships(X, y)\n",
    "corr_with_target = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "top_features = corr_with_target.head(4).index.tolist()\n",
    "pairwise_plot = plot_pairwise_relationships(X, y, top_features)\n",
    "outlier_plot = plot_outliers(X)\n",
    "\n",
    "# Configure the XGBoost model\n",
    "reg = xgb.XGBRegressor(\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='rmse',\n",
    ")\n",
    "\n",
    "# Create train/test split to properly evaluate the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7722)\n",
    "\n",
    "# Train the model with evaluation\n",
    "reg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate predictions for residual plot\n",
    "y_pred = reg.predict(X_test)\n",
    "residual_plot = plot_boxplots(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f02cc5-aef2-4c07-9d4c-95a446f899af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Log the model using MLflow\n",
    "\n",
    "When you log a model using MLflow on Databricks, important artifacts and metadata are captured. This ensures that your model is not only reproducible but also ready for deployment with all necessary dependencies and clear API contracts. For details on what is logged, see the [MLflow documentation](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.xgboost.html).  \n",
    "\n",
    "The code in the next cell starts an MLflow run using `with mlflow.start_run():`. This initializes the MLflow context manager for the run and encloses the run in a code block. When the code block ends, all logged metrics, parameters, and artifacts are saved, and the MLflow run is automatically terminated.\n",
    "\n",
    "In MLflow 3.0, calling `mlflow.xgboost.log_model()` creates a logged model object with all associated metrics and parameters that can be accessed across runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9f005b-d829-453a-bc56-a666d98bef9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Incorporate MLflow evaluation\n",
    "evaluation_data = X_test.copy()\n",
    "evaluation_data[\"label\"] = y_test\n",
    "\n",
    "# Log the model and training metadata results\n",
    "with mlflow.start_run() as run:\n",
    "    # Extract metrics\n",
    "    final_train_rmse = np.array(reg.evals_result()[\"validation_0\"][\"rmse\"])[-1]\n",
    "    final_test_rmse = np.array(reg.evals_result()[\"validation_1\"][\"rmse\"])[-1]\n",
    "    \n",
    "    # Extract parameters for logging\n",
    "    feature_map = {key: value for key, value in reg.get_xgb_params().items() if value is not None}\n",
    "\n",
    "    # Generate a model signature using the infer_signature utility in MLflow\n",
    "    # A signature is required to register the model to Unity Catalog \n",
    "    # so that the model can be used in SQL queries\n",
    "    signature = infer_signature(X, reg.predict(X))\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params(feature_map)\n",
    "    \n",
    "    # Log the model to MLflow and register the model to Unity Catalog\n",
    "    # All model metrics and parameters will be available in Unity Catalog\n",
    "    model_info = mlflow.xgboost.log_model(\n",
    "        xgb_model=reg,\n",
    "        name=\"xgboost_regression_model\",\n",
    "        input_example=X.iloc[[0]],\n",
    "        signature=signature,\n",
    "        registered_model_name=\"main.default.xgboost_regression_model\",\n",
    "    )\n",
    "\n",
    "    # Log metrics to the run and model\n",
    "    mlflow.log_metric(\"train_rmse\", final_train_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", final_test_rmse)\n",
    "    \n",
    "    # Log feature analysis plots\n",
    "    # Plots are saved as artifacts in MLflow\n",
    "    mlflow.log_figure(dist_plot, \"feature_distributions.png\")\n",
    "    mlflow.log_figure(corr_plot, \"correlation_heatmap.png\")\n",
    "    mlflow.log_figure(scatter_plot, \"feature_target_relationships.png\")\n",
    "    mlflow.log_figure(pairwise_plot, \"pairwise_relationships.png\")\n",
    "    mlflow.log_figure(outlier_plot, \"outlier_detection.png\")\n",
    "    mlflow.log_figure(residual_plot, \"feature_boxplots_by_target.png\")\n",
    "        \n",
    "    # Plot feature importance\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    xgb.plot_importance(reg, ax=ax, importance_type='gain')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")\n",
    "\n",
    "    # Run MLflow evaluation to generate additional metrics without having to implement them\n",
    "    mlflow.models.evaluate(\n",
    "        model=model_info.model_uri, \n",
    "        data=evaluation_data, \n",
    "        targets=\"label\", \n",
    "        model_type=\"regressor\", \n",
    "        evaluator_config={\"metric_prefix\": \"mlflow_evaluation_\"},\n",
    "    )\n",
    "    \n",
    "    print(f\"Model logged: {model_info.model_uri}\")\n",
    "    print(f\"Train RMSE: {final_train_rmse:.4f}\")\n",
    "    print(f\"Test RMSE: {final_test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621a9abe-4d05-4afb-9c6d-a08af54d1d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Hyperparameter tuning\n",
    "\n",
    "This section shows how to automate hyperparameter tuning using [Optuna](https://optuna.org/) and [nested runs in MLflow](https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/part1-child-runs/). In this way you can explore a range of parameter configurations and capture all of the experimental details.\n",
    "\n",
    "The code in the next cell does the following:\n",
    "1. Uses the `create_regression_data` function defined previously to generate a synthetic regression dataset.\n",
    "2. Splits the dataset into separate training and test datasets, and saves a copy of the test dataset for evaluation.\n",
    "3. Trains the XGBoost regression model.\n",
    "4. Creates an objective function for the hyperparameter tuning process. The objective function defines the search space for hyperparameters of the XGBoost regressor, such as the maximum tree depth, number of estimators, learning rate, and sampling ratios. Optuna dynamically samples these values, ensuring that each trial tests a different combination of parameters.\n",
    "5. Initiates a nested MLflow run inside the objective function. This nested run automatically captures and logs all details specific to the current hyperparameter trial. By isolating each trial in its own nested run, you can keep a well-organized record of each configuration and its corresponding performance metrics. The nested run logs the following:   \n",
    "    - The specific hyperparameters used for that trial.\n",
    "    - The performance metric (in this case, RMSE) computed on the test set.\n",
    "    - The trained model instance is also stored as part of the trial’s metadata, allowing easy retrieval of the best-performing model later.  \n",
    "\n",
    "    The code does not record each model to MLflow. While doing hyperparameter tuning, each iteration is not guaranteed to be particularly good, so there is no reason to record the model artifact for each one.\n",
    "\n",
    "6. Create a parent MLflow run. This run initiates an Optuna study designed to identify the optimal set of hyperparameters (the set that produces the minimum RMSE). Optuna runs a series of trials where each trial uses a unique combination of hyperparameters. During each trial, the nested MLflow run captures all the experiment details, so you can later track and compare the performance of each model configuration.\n",
    "7. The study identifies the best trial based on the lowest RMSE. The code logs the metrics, parameters, and model from the best trial, and registers the best model in Unity Catalog. The code uses `infer_signature` to save a model signature, which specifies the expected input and output schemas and is important for consistent deployment and integration with systems like Unity Catalog. Finally, additional artifacts such as EDA plots and feature importance charts are recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6c83e8-58f6-4486-ba45-4178f7daf71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import xgboost as xgb\n",
    "\n",
    "# Generate training and validation data\n",
    "n_samples = 2000\n",
    "n_features = 10\n",
    "\n",
    "X, y = create_regression_data(n_samples=n_samples, n_features=n_features, nonlinear=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the evaluation data\n",
    "evaluation_data = X_test.copy()\n",
    "evaluation_data[\"label\"] = y_test\n",
    "\n",
    "# The objective function defines the search space for the key hyperparameters of the XGBRegressor algorithm.\n",
    "# Optuna dynamically samples these values, so that each trial tests a different combination of parameters.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\"\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(param)\n",
    "        regressor = xgb.XGBRegressor(**param)\n",
    "        regressor.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "        preds = regressor.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    # Store the model in the trial's `user attributes`\n",
    "    trial.set_user_attr(\"model\", regressor)\n",
    "    return rmse\n",
    "\n",
    "# In the parent run, save the best iteration from the hyperparameter tuning execution\n",
    "with mlflow.start_run():\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_model = best_trial.user_attrs[\"model\"]\n",
    "\n",
    "    mlflow.log_metric(\"best_rmse\", best_trial.value)\n",
    "    mlflow.log_params(best_trial.params)\n",
    "\n",
    "    signature = infer_signature(X_train, best_model.predict(X_test))\n",
    "\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=best_model,     \n",
    "        name=\"xgboostoptuna\",\n",
    "        input_example=X_train.iloc[[0]],\n",
    "        signature=signature,\n",
    "        model_format=\"ubj\",\n",
    "        registered_model_name=\"main.default.xgboostoptuna\",\n",
    "    )\n",
    "\n",
    "    mlflow.models.evaluate(\n",
    "        model=model_info.model_uri, \n",
    "        data=evaluation_data, \n",
    "        targets=\"label\", \n",
    "        model_type=\"regressor\", \n",
    "        evaluator_config={\"metric_prefix\": \"mlflow_evaluation_\"},\n",
    "    )\n",
    "\n",
    "    dist_plot = plot_feature_distributions(X_train, y_train)\n",
    "    corr_plot = plot_correlation_heatmap(X_train, y_train)\n",
    "    scatter_plot = plot_feature_target_relationships(X_train, y_train)\n",
    "\n",
    "    # Select a few interesting features for the pairwise plot\n",
    "    # Choose features with highest correlation with target\n",
    "    corr_with_target = X_train.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "    top_features = corr_with_target.head(4).index.tolist()\n",
    "    pairwise_plot = plot_pairwise_relationships(X, y, top_features)\n",
    "\n",
    "    # Log the plots associated with the parent run only\n",
    "    mlflow.log_figure(dist_plot, \"feature_distributions.png\")\n",
    "    mlflow.log_figure(corr_plot, \"correlation_heatmap.png\")\n",
    "    mlflow.log_figure(scatter_plot, \"feature_target_relationships.png\")\n",
    "    mlflow.log_figure(pairwise_plot, \"pairwise_relationships.png\")\n",
    "    mlflow.log_figure(outlier_plot, \"outlier_detection.png\")\n",
    "    mlflow.log_figure(residual_plot, \"feature_boxplots_by_target.png\")\n",
    "        \n",
    "    # Plot feature importance of the best model only\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    xgb.plot_importance(best_model, ax=ax, importance_type='gain')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2c394a-3c83-407b-b65d-5300b5378120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Assign a human-readable alias\n",
    "\n",
    "MLflow provides the capability to assign human-readable aliases to registered models.\n",
    "\n",
    "When you set an alias for a model, you create a meaningful label—such as \"best\" or \"production\"—to a specific version of the model. This label makes it easy for team members and automated systems to identify which version is intended for deployment or further evaluation.\n",
    "\n",
    "Many of the model registry APIs recognize and work with these aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "affe45a1-f74b-4d26-9b49-5c483702ad15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the `MlflowClient` to access metadata, artifacts, and information about models that are tracked or registered to the model registry.\n",
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Set the alias on the desired version. This example uses version 1.\n",
    "client.set_registered_model_alias(\"main.default.xgboostoptuna\", \"best\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd8c19d-5878-4536-8e45-5fb95771ee2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Pre-deployment validation\n",
    "\n",
    "MLflow provides the `mlflow.models.predict` utility to simulate a production-like environment and validate that your model is configured correctly.\n",
    "\n",
    "`mlflow.models.predict` fetches your logged model from the specified model URI, validates any dependencies, and builds a virtual execution environment within a subprocess. This simulates deploying your model on a virtual machine, closely mirroring a production model serving scenario.\n",
    "\n",
    "The utility supports multiple environment managers to build the execution environment. For more information, see the [MLflow documentation](https://mlflow.org/docs/latest/model/#environment-managers). This example uses `uv`, which is recommended for best performance. This notebook installed the package in the first cell. \n",
    "\n",
    "You can supply data for inference in two ways:\n",
    "- **In-memory data:**  \n",
    "  Pass an in-memory object to the `input_data` argument. This allows for immediate validation within your notebook.\n",
    "- **External data location:**  \n",
    "  Alternatively, use the `input_path` argument to specify a location (such as a volume in Unity Catalog) from which to read the data.\n",
    "\n",
    "To keep a record of the model’s predictions, specify an `output_path` to save the inference results. If you don’t specify an output path, the prediction results are displayed in the cell's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30465c12-336c-4ae8-b5cc-7c5eba2ac3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri = \"models:/main.default.xgboostoptuna@best\"\n",
    "\n",
    "mlflow.models.predict(model_uri=model_uri, input_data=X_train, env_manager=\"uv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36956452-0aca-47c7-b0f4-d8943814b8cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Load the registered model and make predictions\n",
    "\n",
    "The code in this section shows how to load the registered model from MLflow and use it to make predictions locally. The model URI is based on the alias defined previously. By using the alias in the model URI, you ensure that the most recent version of the model with that alias is used for inference.\n",
    "\n",
    "After the model is loaded, you can generate predictions on your data. This is a confirmation step to confirm that the model is working as expected before you use it for larger-scale applications such as batch prediction. \n",
    "\n",
    "### Should I use PyFunc or native XGBoost?\n",
    "\n",
    "When working with MLflow, you have two primary methods for loading your logged models: the generic `pyfunc` interface and the native XGBoost object. The best choice depends on your use case.\n",
    "\n",
    "- The `pyfunc` interface provides a standardized, framework-agnostic way to interact with your model. This makes it the best choice for real-time model serving and production environments, where a consistent API is required. By using `pyfunc`, your model is encapsulated in a generic wrapper that exposes a simple `predict()` method, ensuring seamless integration and consistent behavior across various deployment scenarios.\n",
    "\n",
    "- Alternatively, you can load the model using `mlflow.xgboost.load_model()`, which returns a native XGBoost object. This method preserves the full functionality of the XGBoost library, allowing you to take advantage of its specialized methods and optimizations. For local validation or batch inference tasks, the native object can offer performance benefits and more granular control during evaluation. However, this approach is less suitable for deployment to production environments that require a standardized interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a862fbcf-fbc1-4344-937b-2a452f663fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the model and use the model to make predictions locally\n",
    "loaded_registered_model = mlflow.pyfunc.load_model(model_uri=model_uri)\n",
    "\n",
    "loaded_registered_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb98730d-1668-4c7a-a9cc-f3313b86387a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Batch prediction using Spark UDF in MLflow\n",
    "\n",
    "The code in this section shows how to perform distributed batch predictions using the Spark UDF integration in MLflow. This approach allows you to leverage the scalable data processing capabilities of Spark to apply your model across large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add7bf88-0bbe-4ca1-b211-5970ac19cb9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the training data into a Spark DataFrame.\n",
    "X_spark = spark.createDataFrame(X_train)\n",
    "\n",
    "# Create a Spark UDF to apply the model to the Spark DataFrame.\n",
    "# Note that `model_uri` is defined based on a model alias, ensuring that you always load the current, approved version.\n",
    "udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri=model_uri,\n",
    ")\n",
    "\n",
    "# Apply the Spark UDF to the DataFrame. This performs batch predictions across all rows in a distributed manner. \n",
    "X_spark = X_spark.withColumn(\"prediction\", udf(*X_train.columns))\n",
    "\n",
    "# Display the resulting DataFrame. \n",
    "display(X_spark)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLFlow experiment management",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
